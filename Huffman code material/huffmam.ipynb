{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Huffman Compression Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By Todor Nedkovski"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook includes a brief explanation about Huffman Compression Algorithm works.\n",
    "\n",
    "We are going to talk about a couple of questions:\n",
    "\n",
    "1. What is the difference betwenn lossless and lossy compression?\n",
    "2. How are Huffman trees constructed?\n",
    "3. How can we get back the uncompressed data from the Huffman tree?\n",
    "4. Comparison with other lossless compression algorithms.\n",
    "\n",
    "In the end we are going to overview what we have just learned and think of other problems we might encounter while compressing files. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the early days computers (around 1980s) did not have much more then a 10 MB storage capacity. It's really mind blowing that you have ten thousand times as much now in your phone then you did back then. The problem is that the files we want to store seem to keep pace with that growth. How can we store more data with less space? The answear is compression! There are a lot of compression algorithms but we are going to talk about a specific one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the difference bewtween Fixed and Variable length code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a Fixed Length code "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a fixed-length code each codeword has the same number of bits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Variable-Length code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In coding theory a variable-length code is a code which maps source symbols to a variable number of bits. There are 3 sub-types of the variable-length code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-singular"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A code is non-singular if each character is mapped to a different bit string. In other words the mapping from characters to bit strings is injective (one to one).\n",
    "\n",
    "1. For example, the mapping $M_{1}=\\{\\,a\\mapsto 0,b\\mapsto 0,c\\mapsto 1\\,\\}$ is not non-singular because both \"a\" and \"b\" map to the same bit string \"0\". Any extension of this mapping will generate a lossy coding. It may be useful when some loss of information is acceptable.\n",
    "2. However, the mapping $M_{2}=\\{\\,a\\mapsto 1,b\\mapsto 011,c\\mapsto 01110,d\\mapsto 1110,e\\mapsto 10011,f\\mapsto 0\\}$ is non-singular."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uniquely decodable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A code is **uniquely decodable** if its extension is non-singular.\n",
    "\n",
    "1. The mapping $M_{3}=\\{\\,a\\mapsto 0,b\\mapsto 01,c\\mapsto 011\\,\\}$ is uniquely decodable. \n",
    "2. Consider again the code $M_{2}$ from the previous section. This code is not uniquely decodable, since the string 011101110011 can be interpreted as the sequence of codewords 01110 – 1110 – 011, but also as the sequence of codewords 011 – 1 – 011 – 10011. The two decodings of this encoded string are cdb and babe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prefix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For something to be a prefix code, the entire set of possible encoded values must not contain any values that start with any other value in the set. For example: $M_{4}=\\{\\,a\\mapsto 0,b\\mapsto 10,c\\mapsto 110\\,\\}$ is a prefix code, because none of the values start with any of the other values. However, $M_{3}=\\{\\,a\\mapsto 0,b\\mapsto 01,c\\mapsto 011\\,\\}$ is not a prefix code, because one of the values starts with another of the values.\n",
    "\n",
    "Prefix codes are useful because you can pick out each value without needing to know where the value starts and ends."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the difference betwenn lossless and lossy compression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is lossless compression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Lossless compression method is capable of reconstituting the original form of the data.\n",
    "The quality of the data is not compromised.\n",
    "This technique allows a file to restore its original form.\n",
    "Lossless compression can be applied to any file format can improve the performance of the compression ratio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is lossy compression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Lossy compression method eliminates some amount of data that is not noticeable.\n",
    "This technique **does not** allow a file to restore in its original form but significantly reduces the size.\n",
    "The lossy compression technique is beneficial if the quality of the data is not your priority.\n",
    "It slightly degrades the quality of the file or data but is convenient when one wants to send or store the data.\n",
    "This type of data compression is used for organic data like audio signals and images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When can we get away with lossy compression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the definition above states lossy compression eliminates the data that is not noticeable.\n",
    "This is said because for the most part we use lossy compression for images, videos and audio files.\n",
    "Most of the information is still there and you would not notice a few milliseconds missing from the video or a couple of pixels missing from the image.\n",
    "Basically everything without text. Unless you are into guessing missing chars from words I guess it is fine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Huffman Compression Algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huffman Compression Algorithm is a prefix loseless type of compression algorithm developed by David A. Huffman (student at MIT), and published in the 1952 paper. I guess you can say it is old. The main idea is to use smaller sequences of bits for more frequent charecters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin generating the Huffman tree, each character gets a weight equal to the number of times it\n",
    "occurs in the file. For example, in the \"happy hip hop\" example, the character 'p' has weight 4,\n",
    "'h' has weight 3, the space has weight 2, and the other characters have weight 1. Our first task is to\n",
    "calculate these weights.\n",
    "\n",
    "1. Create a collection of singleton trees, one for each character, with weight equal to the character frequency.\n",
    "2. From the collection, pick out the two trees with the smallest weights and remove them. Combine them into a new tree whose root has a weight equal to the sum of the weights of the two nodes.\n",
    "3. Add the new tree back into the collection.\n",
    "4. Repeat steps 2 and 3 until there is only one tree left.\n",
    "\n",
    "<img src=\"imgs/first.png\" style=\"width: 400px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by choosing the two smallest nodes. There are four nodes with the minimal weight of one, it\n",
    "doesn't matter which two we pick. We choose 'o' and 'y' and combine them into a new tree whose\n",
    "root is the sum of the weights chosen. We replace those two nodes with the combined tree. The nodes\n",
    "remaining in the collection are shown in the light gray box at each stage.\n",
    "\n",
    "<img src=\"imgs/second.png\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we repeat that step, this time there is no choice for the minimal nodes, it must be 'a' and 'i'.\n",
    "We take those out and combine them into a weight 2 tree. Note how the collection of nodes shrinks by\n",
    "one each iteration (we remove two nodes and add a new one back in).\n",
    "\n",
    "<img src=\"imgs/third.png\" style=\"width: 400px;\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we pull out the two smallest nodes and build a tree of weight 4:\n",
    "\n",
    "<img src=\"imgs/fourth.png\" style=\"width: 400px;\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more iteration combines the weight 3 and 2 trees into a combined tree of weight 5:\n",
    "\n",
    "<img src=\"imgs/fifth.png\" style=\"width: 400px;\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining the two 4s gets a tree of weight 8:\n",
    "\n",
    "<img src=\"imgs/sixth.png\" style=\"width: 400px;\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, we combine the last two to get our final tree. The root node of the final tree will always\n",
    "have a weight equal to the number of characters in the input file.\n",
    "\n",
    "<img src=\"imgs/seventh.png\" style=\"width: 400px;\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that it is essential that you use the same tree to do both encoding and decoding of your files.\n",
    "Since each Huffman tree creates a unique encoding of a particular file, you need to ensure that your\n",
    "decoding algorithm generates the exact same tree, so that you can get back the file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Pseudo-EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now see that our three has asymmetrical form (jagged tree). We have not talked about this but trees usually should not be like this. What happens when we try to store a Huffman-encoded sequence on-disk in a file? Each file on your computer is typically stored as a number of bytes (groups of eight bits). Therefore if you try to write a Huffman-encoded string of bits into a file and you don't have exactly multiple of eight bits then your operating system would add random bits at the end. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example the word \"ahoy\" with the huffman tree above translated would look something like this \n",
    "\n",
    "<center><b>1101001100111</b></center>\n",
    "\n",
    "Those are excatly thirheen bits which means that 3 more will be added. Let's say that those 3 bits will be **111**\n",
    "\n",
    "In that case, the bit sequence would be written to disk as\n",
    "\n",
    "<center><b>1101001100111111</b></center>\n",
    "\n",
    "If we were to then load this back from disk and decode it into a sequence of characters, we would get \"ahoyi\". Even worse if those random bits are not in out tree (in that case \"000\") we would get an error.\n",
    "\n",
    "This is clearly a problem. We need a special charcter which tells us where our compressed data ends. It won't be seen and we can add it to our tree in the same way we have added all the other characters. Here is a possible tree with ■\n",
    "\n",
    "<img src=\"imgs/eight.png\" style=\"width: 400px;\">\n",
    "\n",
    "If we encode \"happy hip hop■\" we get the following bitstring\n",
    "\n",
    "<center><b>001100101011110110011011001100111010010</b></center>\n",
    "\n",
    "| Symbol | Codeword |\n",
    "|--------|----------|\n",
    "| h      | 00       |\n",
    "| a      | 1100     |\n",
    "| p      | 10       |\n",
    "| p      | 10       |\n",
    "| y      | 1111     |\n",
    "|        | 011      |\n",
    "| h      | 00       |\n",
    "| i      | 1101     |\n",
    "| p      | 10       |\n",
    "|        | 011      |\n",
    "| h      | 00       |\n",
    "| o      | 1110     |\n",
    "| p      | 10       |\n",
    "| ■      | 010      |\n",
    "| ignore | ignore   |\n",
    "\n",
    "This ■ is called *pseudo end of file* or *Pseudo-EOF* for short."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here we are going to write some code so it would be better if we gather all of our imports in one place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement it with python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create our Node tree class\n",
    "\n",
    "```python\n",
    "class NodeTree(object):\n",
    "\n",
    "    def __init__(self, left=None, right=None):\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "\n",
    "    def children(self):\n",
    "        return (self.left, self.right)\n",
    "\n",
    "    def nodes(self):\n",
    "        return (self.left, self.right)\n",
    "\n",
    "    def __str__(self):\n",
    "        return '%s_%s' % (self.left, self.right)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to calculate character's frequences.\n",
    "\n",
    "```python\n",
    "def calculate_frequency(data):\n",
    "    freq = {}\n",
    "    for c in data:\n",
    "        if c in freq:\n",
    "            freq[c] += 1\n",
    "        else:\n",
    "            freq[c] = 1\n",
    "    return freq\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that we need to sort them in ascending order\n",
    "\n",
    "```python\n",
    "freq = sorted(freq.items(), key=lambda x: x[1], reverse=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to create the function that merges our nodes  \n",
    "\n",
    "```python\n",
    "def merge(nodes)\n",
    "    while len(nodes) > 1:\n",
    "        (key1, c1) = nodes[-1]\n",
    "        (key2, c2) = nodes[-2]\n",
    "        nodes = nodes[:-2]\n",
    "        node = NodeTree(key1, key2)\n",
    "        nodes.append((node, c1 + c2))\n",
    "\n",
    "        nodes = sorted(nodes, key=lambda x: x[1], reverse=True)\n",
    "    return nodes\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we need to add the nodes to the dictionary\n",
    "\n",
    "```python\n",
    "def huffman_code_tree(node, left=True, binString=''):\n",
    "    if type(node) is str:\n",
    "        return {node: binString}\n",
    "    (l, r) = node.children()\n",
    "    d = dict()\n",
    "    d.update(huffman_code_tree(l, True, binString + '0'))\n",
    "    d.update(huffman_code_tree(r, False, binString + '1'))\n",
    "    return d\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All combined it should look something like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class NodeTree(object):\n",
    "\n",
    "    def __init__(self, left=None, right=None):\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "\n",
    "    def children(self):\n",
    "        return (self.left, self.right)\n",
    "\n",
    "    def nodes(self):\n",
    "        return (self.left, self.right)\n",
    "\n",
    "    def __str__(self):\n",
    "        return '%s_%s' % (self.left, self.right)\n",
    "\n",
    "def calculate_frequency(data):\n",
    "    freq = {}\n",
    "    for c in data:\n",
    "        if c in freq:\n",
    "            freq[c] += 1\n",
    "        else:\n",
    "            freq[c] = 1\n",
    "    return freq\n",
    "\n",
    "def huffman_code_tree(node, left=True, binString=''):\n",
    "    if type(node) is str:\n",
    "        return {node: binString}\n",
    "    (l, r) = node.children()\n",
    "    d = dict()\n",
    "    d.update(huffman_code_tree(l, True, binString + '0'))\n",
    "    d.update(huffman_code_tree(r, False, binString + '1'))\n",
    "    return d\n",
    "\n",
    "def merge(nodes):\n",
    "    while len(nodes) > 1:\n",
    "        (key1, c1) = nodes[-1]\n",
    "        (key2, c2) = nodes[-2]\n",
    "        nodes = nodes[:-2]\n",
    "        node = NodeTree(key1, key2)\n",
    "        nodes.append((node, c1 + c2))\n",
    "\n",
    "        nodes = sorted(nodes, key=lambda x: x[1], reverse=True)\n",
    "    return nodes\n",
    "\n",
    "def encode(text, table):\n",
    "    result = \"\"\n",
    "    \n",
    "    for c in text:\n",
    "        result += table[c]\n",
    "    return result\n",
    "\n",
    "def decode(encoded_text, tree):\n",
    "    decoded_text = \"\"\n",
    "    \n",
    "    buffer = \"\"\n",
    "    \n",
    "    for i in encoded_text:\n",
    "        buffer += i\n",
    "        if buffer in tree.values():\n",
    "            value = str([k for k, v in tree.items() if v == buffer][0])\n",
    "            decoded_text += value\n",
    "            buffer = \"\"\n",
    "    return decoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress(string):\n",
    "    freq = calculate_frequency(string)\n",
    "    \n",
    "    freq = sorted(freq.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    nodes = freq\n",
    "\n",
    "    nodes = merge(nodes)\n",
    "\n",
    "    huffmanCode = huffman_code_tree(nodes[0][0])\n",
    "        \n",
    "    return (huffmanCode, freq, encode(string, huffmanCode))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And of course a function to uncompress it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uncompress(encoded_text, tree):\n",
    "    return decode(encoded_text, tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's also good to have a function to print the encoding tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tree(huffmanCode, freq):\n",
    "    print(' Char | Huffman code ')\n",
    "    print('----------------------')\n",
    "    for (char, frequency) in freq:\n",
    "        print(' %-4r |%12s' % (char, huffmanCode[char]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement data reading function in advance because we are going to use it a lot later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(file):\n",
    "    data = open(\"data/\" + file + \".txt\")\n",
    "    text = data.readlines()\n",
    "    return text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = get_data(\"non_random_text\")\n",
    "(code, freq, encoding) = compress(text)\n",
    "uncompress(encoding, code) == text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the code above we see that our uncompress function works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text = get_data(\"non_random_text\")\n",
    "(code, freq, encoding) = compress(text)\n",
    "print_tree(code, freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see how the most common character, in this case \"e\", is encoded with fewer bits because it's frequency is higher . As for \"K\" is encoded with more bits because it's frequency is lower . I wonder what would happen if we give more \"random\" data to our algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text = get_data(\"random_text\")\n",
    "(code, freq, encoding) = compress(text)\n",
    "print_tree(code, freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting... I expected more uneven distibution of the frequencies. I wonder if there is correlation between character count and the randomness of our data. We can easily calculate the randmness but first let's see the compressed/uncompressed ratio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need a function to count the number of bits in a specific code tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bits(code, freq):\n",
    "    bits = 0\n",
    "    for (char, count) in freq:\n",
    "        bits += len(code[char]) * count\n",
    "    return bits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can calculate how much data we can compress with that particular code tree with the function below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparisson(file):\n",
    "    text = get_data(file)\n",
    "\n",
    "    (code, freq, encode) = compress(text)\n",
    "\n",
    "    after_compression = calculate_bits(code, freq)\n",
    "\n",
    "    before_compression = len(bin(int.from_bytes(get_data(\"non_random_text\").encode(), 'big')))\n",
    "\n",
    "    percentage = (after_compression / before_compression) * 100\n",
    "\n",
    "    print(str(before_compression) + \" bits before\")\n",
    "    print(str(after_compression) + \" bits after\")\n",
    "    print(\"%.2f\" % percentage + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "comparisson(\"non_random_text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That means that we have $45.52\\%$ compressed space of the original file. Let's try that with the random data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparisson(\"random_text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It really should not be surprising that we have only $19.95\\%$ comparison. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can defenetly say that more unpredictable data is less compressed. We don't know why yet but soon we might."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is the file \"random_text\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_data(\"non_random_text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that is the file \"non_random_text\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_data(\"random_text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is entropy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entropy or more specifically Shannon entropy helps us calculate what is the \"randomness\" of our distribution in our case text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information for an Event"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intuition behind quantifying information is the idea of measuring how much surprise there is in an event. Those events that are rare (low probability) are more surprising and therefore have more information than those events that are common (high probability).\n",
    "\n",
    "In other words:\n",
    "\n",
    "1. **Low Probability Event**: High Information (surprising).\n",
    "2. **High Probability Event**: Low Information (unsurprising).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$h(p_i) = - \\log_{2}{p_i}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log is on base 2 because we want to measure our data in bits. The negative sign ensures that the result is always positive or zero. Consider a flip of a single fair coin. The probability of heads (and tails) is 0.5. Let's make some examples with python!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def information_for_p(p):\n",
    "    p = p\n",
    "    h = - np.log(p) / np.log(2)\n",
    "    print('p(x)=%.3f, information: %.3f bits' % (p, h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "information_for_p(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the probability of our event is 0.5 it's information content for the event is 1 bit. If we flip that let's say 100 times the information for this sequence would be 100 bits. (That's called foreshadowing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider our coin is not fair and the chance of getting heads is 0.9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "information_for_p(0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here our event is more frequent and thus it's information size is smaller."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the function $-log_2p_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "\n",
    "info = [-np.log(p) / np.log(2) for p in probs]\n",
    "\n",
    "plt.plot(probs, info, marker='.')\n",
    "plt.title('Probability vs Information')\n",
    "plt.xlabel('Probability')\n",
    "plt.ylabel('Information')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should not be surprised by two things the first of which is that the graph is not linear. It's normall because our function is log and the second is the correlation bewteen information size and the it's probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the Entropy for a Random Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(probabilities, base):\n",
    "    h = 0\n",
    "    for p in probabilities:\n",
    "        h += -np.log(p) / np.log(base) * p\n",
    "    return h "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = [1/6, 1/6, 1/6, 1/6, 1/6, 1/6]\n",
    "print('entropy: %.3f bits' % entropy(p, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mathematically written it should look something like that \n",
    "\n",
    "$$H(x) = -\\sum_{i = 1}^n p_i {\\log_2{p_i}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are able to calculate the entropy of our distrubution. We would also need a function to calculate characters freqency and another function to calculate the probability of any single one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_probability(frequencies, base_count):\n",
    "    prob = []\n",
    "    for f in frequencies.values():\n",
    "        prob.append(f/base_count)\n",
    "    return prob\n",
    "\n",
    "def entropy(data, base):\n",
    "    frequencies = calculate_frequency(data)\n",
    "    probabilities = calculate_probability(frequencies, len(data))\n",
    "    \n",
    "    h = 0\n",
    "    \n",
    "    for p in probabilities:\n",
    "        h += (np.log(p) / np.log(base)) * p\n",
    "        \n",
    "    return -h "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = get_data(\"random_text\")\n",
    "base = 2\n",
    "\n",
    "entropy(string, base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = get_data(\"non_random_text\")\n",
    "base = 2\n",
    "\n",
    "entropy(string, base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our random text it's entrophy is approximately 6.35 which means that on average we need 6.35 bits to represent a single character. With that said with big entropy comes unpredictability which is a synonym for randomness. Same for small entrophy. Small entrophy means more predictability and you know the rest. Side note (at first I thought that there is going to be correlation between characters frequencies and the entrophy of our data but there is none)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have come to the conclusion that Huffman coding uses entropy encoding scheme."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How can we get back the uncompressed data from the Huffman tree?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One problem we will encounter while trying to send the compressed data is that the recivier does not have the tree to recover the data. There a couple of ways to resove this. We could agree on the coding tree in advance but for that we need to know the frequency of the letters in advance. Second option is to prefix the bit sequence with a header containing enough information to reconstruct the Huffman encoding tree. You can store that sequence at the head of the file in a human readable format. Reading this it will allow you to recreate the the tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison with other lossless compression algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use an already done comparisson. We have got the files sizes here. But first let's define what all compression algorithms mentioined below mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run-length encoding (RLE)** is a form of lossless data compression in which runs of data (sequences in which the same data value occurs in many consecutive data elements) are stored as a single data value and count, rather than as the original run. For example this string \"aaaaaeeeeeffff\" translates to \"5a5e4f\". As you can see this works only if we have consecutive characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Shannon Fano Algorithm** is an entropy encoding technique for lossless data compression of multimedia. Named after Claude Shannon and Robert Fano, it assigns a code to each symbol based on their probabilities of occurrence. It is a variable length encoding scheme, that is, the codes assigned to the symbols will be of varying length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adaptive Huffman coding** (also called **Dynamic Huffman coding**) is an adaptive coding technique based on Huffman coding but with the expetion that it allows building the tree as the symbols are being send having no knowledge of charcter's frequencies. It's used in live video and audio compressions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Arithmetic coding** is a form of entropy encoding used in lossless data compression. Normally, a string of characters such as the words \"hello there\" is represented using a fixed number of bits per character, as in the ASCII code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs\\datasize.png\" style=\"width: 500px;\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs\\comparison.png\" style=\"width: 700px;\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall behaviour of Shannon-Fano coding, Huffman coding and Adaptive Huffman coding is very similar with Arithmetic coding. Unlike Huffman coding, no code tree needs to be transmitted to the receiver. Here, encoding is done to a group of symbols, not symbol by symbol, which leads to higher compression ratios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have talked about what huffman coding is, how is generated. We made a wrong hypothesis that data with evenly distrubuted character counts is not random. We have talked about entrophy and how and why exactly we mesure one's data randomness. We made a test that confirms it. This is supposed to be a brief explanation about Huffman coding and it's properties.       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [https://en.wikipedia.org/wiki/Huffman_coding](https://en.wikipedia.org/wiki/Huffman_coding)\n",
    "2. [https://www.youtube.com/watch?v=dM6us854Jk0](https://www.youtube.com/watch?v=dM6us854Jk0)\n",
    "3. [https://web.stanford.edu/class/archive/cs/cs106b/cs106b.1126/handouts/220%20Huffman%20Encoding.pdf](https://web.stanford.edu/class/archive/cs/cs106b/cs106b.1126/handouts/220%20Huffman%20Encoding.pdf)\n",
    "4. [http://160592857366.free.fr/joe/ebooks/ShareData/A%20Comparitive%20Study%20of%20Text%20Compression%20Algorithms.pdf](http://160592857366.free.fr/joe/ebooks/ShareData/A%20Comparitive%20Study%20of%20Text%20Compression%20Algorithms.pdf)\n",
    "5. [http://home.cse.ust.hk/faculty/golin/COMP271Sp03/Notes/MyL17.pdf](http://home.cse.ust.hk/faculty/golin/COMP271Sp03/Notes/MyL17.pdf)\n",
    "6. [https://www.programiz.com/dsa/huffman-coding](https://www.programiz.com/dsa/huffman-coding) - Huffman source code\n",
    "7. [https://en.wikipedia.org/wiki/Prefix_code](https://en.wikipedia.org/wiki/Prefix_code)\n",
    "8. [https://www.tutorialspoint.com/huffman-codes-and-entropy-in-data-structure](https://www.tutorialspoint.com/huffman-codes-and-entropy-in-data-structure)\n",
    "9. [https://machinelearningmastery.com/what-is-information-entropy/](https://machinelearningmastery.com/what-is-information-entropy/)\n",
    "10. [https://en.wikipedia.org/wiki/Variable-length_code](https://en.wikipedia.org/wiki/Variable-length_code)\n",
    "11. [https://leimao.github.io/blog/Huffman-Coding/](https://leimao.github.io/blog/Huffman-Coding/)\n",
    "12. [https://www.princeton.edu/~cuff/ele201/kulkarni_text/information.pdf](https://www.princeton.edu/~cuff/ele201/kulkarni_text/information.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
